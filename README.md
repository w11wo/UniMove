# UniMoveğŸš¶â€â™‚ï¸ğŸ™ï¸

This is the official PyTorch implementation for our paper, **UniMove: A Unified Model for Multi-city Human Mobility Prediction**, accepted at ACM SIGSPATIAL 2025. 

## âš™ï¸ Installation
### Environment
- Tested OS: Linux
- Python >= 3.11
- torch == 2.0.1
- CUDA == 11.7

### Dependencies:
1. Install Pytorch with the correct CUDA version.
2. Use the `pip install -r requirements.txt` command to install all of the Python modules and packages used in this project.


## âš– Repo Structure

```
UniMove  
â”œâ”€location_feature                  # Location features [N,31], N means number of locations, 31=28(poi feature)+2(longitude & latitude)+1(popularity rank)  
â”‚   â”œâ”€vocab_shanghai.npy        
â”‚   â”œâ”€vocab_nanchang.npy      
â”‚   â””â”€vocab_lasa.npy    
â”‚  
â”œâ”€traj_dataset                      # The dataset examples where each trajectory is formatted as [user_id location,weekday,time;location,weekday,time;...].  
â”‚   â””â”€mini  
â”‚       â”œâ”€test        
â”‚       â”œâ”€train       
â”‚       â””â”€val     
â”‚  
â”œâ”€dataloader.py  
â”œâ”€location_tower_model.py          # The architecture of location tower  
â”œâ”€model.py                         # The architecture of trajectory tower  
â”œâ”€utils.py                         # Train and evaluate methods  
â”œâ”€main.py  
â””â”€requirements.txt
```



## ğŸƒ Model Training
You can train UniMove with multi-city datasets and test with nanchang dataset as the following examples:

```python
python main.py --device cuda:0 --city nanchang shanghai lasa --target_city nanchang
```

Once your model is trained, you will find the logs recording the training process in the  `./logs_{args.city}` directory.

## ğŸ“œ Citation
If you find our work or this repository useful for your research, please consider citing our paper:
```
@article{han2025unimove,
  title={UniMove: A Unified Model for Multi-city Human Mobility Prediction},
  author={Han, Chonghua and Yuan, Yuan and Liu, Yukun and Ding, Jingtao and Feng, Jie and Li, Yong},
  journal={arXiv preprint arXiv:2508.06986},
  year={2025}
}g
```
